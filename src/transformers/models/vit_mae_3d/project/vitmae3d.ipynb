{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minghui/github/transformers/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-13 14:00:04.742150: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-13 14:00:04.742187: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-13 14:00:04.742190: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTMAE3DConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"decoder_hidden_size\": 384,\n",
      "  \"decoder_intermediate_size\": 1536,\n",
      "  \"decoder_num_attention_heads\": 16,\n",
      "  \"decoder_num_hidden_layers\": 8,\n",
      "  \"embed_dim\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 91,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_ratio\": 0.25,\n",
      "  \"model_type\": \"vit_mae_3d\",\n",
      "  \"norm_pix_loss\": false,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 7,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.30.0.dev0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTMAE3DConfig, ViTMAE3DForPreTraining\n",
    "\n",
    "\n",
    "# Initialize a ViT MAE vit-mae-base style configuration\n",
    "config = ViTMAE3DConfig(\n",
    "    image_size=91,\n",
    "    num_channels=1,\n",
    "    patch_size=7,\n",
    "    embed_dim=768,\n",
    "    mask_ratio=0.25\n",
    "    # norm_pix_loss=False\n",
    ")\n",
    "\n",
    "# Initialize a model (with random weights) from the vit-mae-base style configuration\n",
    "vit_mae = ViTMAE3DForPreTraining(config)\n",
    "\n",
    "# # Access model's configuration\n",
    "_configuration = vit_mae.config\n",
    "print(_configuration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from torch.utils.data import Dataset\n",
    "from monai.data import NibabelReader\n",
    "\n",
    "class SPRINT_T1w_flat_Dataset:\n",
    "    def __init__(self, data_dir, filenames, subjects_csv, mode='train', transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        # read all nifti files in data_dir\n",
    "        self.filenames = filenames\n",
    "        self.transform = transform\n",
    "                \n",
    "        # read labels of each subject\n",
    "        self.labels = {}\n",
    "        with open(subjects_csv, \"r\") as fp:\n",
    "            csv_reader = csv.reader(fp, delimiter=\",\")\n",
    "            for row in csv_reader:\n",
    "                if row[0] == \"subject_id\":\n",
    "                    continue\n",
    "                id = row[0]\n",
    "                label = int(row[4])\n",
    "                self.labels[id] = label\n",
    "\n",
    "        # count how many label == 1\n",
    "        progress = 0\n",
    "        not_progress = 0\n",
    "        for filename in self.filenames:\n",
    "            id = re.search(r'subject_(\\d{3})-(\\d{4})', filename).group(2)\n",
    "            if self.labels[id] == 1:\n",
    "                progress += 1\n",
    "            else:\n",
    "                not_progress += 1\n",
    "        print(f\"Total subjects: {len(self.filenames)}, Progressing: {progress}, Not progressing: {not_progress}\")\n",
    "\n",
    "        # create image reader\n",
    "        self.image_reader = NibabelReader()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        # get subject id from filename using regex\n",
    "        id = re.search(r'subject_(\\d{3})-(\\d{4})', filename).group(2)\n",
    "        image = self.image_reader.read(os.path.join(self.data_dir, filename))\n",
    "        \n",
    "        image = image.get_fdata().astype(np.float32)\n",
    "        image = torch.from_numpy(image)\n",
    "        # leave only middle 91 channels in dimension 1\n",
    "        image = image[:, 9:100, :]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        image = rearrange(image, 'd h w -> 1 d h w')\n",
    "        label = self.labels[id]\n",
    "\n",
    "        return {'image': image, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subjects: 751, Progressing: 326, Not progressing: 425\n",
      "torch.Size([2, 1, 91, 91, 91])\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# open files.txt which contains all the file paths\n",
    "with open('/home/minghui/github/NMSS/vitmae/files.txt', 'r') as f:\n",
    "    files = f.readlines()\n",
    "files = [x.strip() for x in files]\n",
    "\n",
    "data_dir = '/media/minghui/Data/Datasets/NMSS Study/yuxin/agg_normalized/'\n",
    "label_csv = '/home/minghui/github/NMSS/vitmae/subject_list.csv'\n",
    "\n",
    "# custom torch transform to select num_channels random channels\n",
    "class RandomChannelSelect:\n",
    "    def __init__(self, num_channels=8):\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # img is a 4D tensor of shape (1, C, H, W)\n",
    "        # randomly select a starting channel\n",
    "        start_channel = np.random.randint(0, img.shape[0] - self.num_channels)\n",
    "        img = img[start_channel:start_channel+self.num_channels, :, :]\n",
    "        return img\n",
    "\n",
    "class RandomCrop3D:\n",
    "    def __init__(self, size=64):\n",
    "        self.size = size\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # img is a 3D tensor of shape (x, y, z)\n",
    "        # randomly select a starting channel\n",
    "        x_start = np.random.randint(0, img.shape[0] - self.size)\n",
    "        y_start = np.random.randint(0, img.shape[1] - self.size)\n",
    "        z_start = np.random.randint(0, img.shape[2] - self.size)\n",
    "        img = img[x_start:x_start+self.size, y_start:y_start+self.size, z_start:z_start+self.size]\n",
    "        return img\n",
    "\n",
    "class RandomDimensionPermute:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # img is a 4D tensor of shape (1, C, H, W)\n",
    "        # randomly permute the dimensions\n",
    "        dims = ['x', 'y', 'z']\n",
    "        np.random.shuffle(dims)\n",
    "        img = rearrange(img, 'x y z -> {}'.format(' '.join(dims)))\n",
    "        return img\n",
    "\n",
    "train_transforms = T.Compose([\n",
    "    # RandomCrop3D(80),\n",
    "    # RandomDimensionPermute(),\n",
    "    # RandomChannelSelect(16),\n",
    "    # T.RandomHorizontalFlip(),\n",
    "    # T.RandomVerticalFlip(),\n",
    "    # T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=(-10, 10, -10, 10)),\n",
    "    # T.RandomAdjustSharpness(0.5),\n",
    "    T.GaussianBlur(3, sigma=(0.05, 0.2)),\n",
    "    # random noise\n",
    "    # T.RandomRotation(90),\n",
    "    # T.RandomCrop(64),\n",
    "])\n",
    "\n",
    "train_dataset = SPRINT_T1w_flat_Dataset(data_dir, files, label_csv, mode='train', transform=train_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)\n",
    "\n",
    "for batch in train_loader:\n",
    "    image = batch['image']\n",
    "    print(image.shape)\n",
    "    label = batch['label']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2441.332264677007\n",
      "Epoch: 1, Loss: 2385.692653899497\n",
      "Epoch: 2, Loss: 2351.358447785073\n",
      "Epoch: 3, Loss: 2432.1738834786925\n",
      "Epoch: 4, Loss: 2126.4023198878513\n",
      "Epoch: 5, Loss: 1551.0795330291098\n",
      "Epoch: 6, Loss: 2320.979242852394\n",
      "Epoch: 7, Loss: 2454.360929773209\n",
      "Epoch: 8, Loss: 2358.950841700777\n",
      "Epoch: 9, Loss: 1059.1752731647898\n",
      "Epoch: 10, Loss: 506.33268802723984\n",
      "Epoch: 11, Loss: 480.0423789328717\n",
      "Epoch: 12, Loss: 462.70029749768844\n",
      "Epoch: 13, Loss: 453.2670467457873\n",
      "Epoch: 14, Loss: 448.8441321190367\n",
      "Epoch: 15, Loss: 444.7785218421449\n",
      "Epoch: 16, Loss: 440.58440220609623\n",
      "Epoch: 17, Loss: 437.936207872756\n",
      "Epoch: 18, Loss: 437.64095898892015\n",
      "Epoch: 19, Loss: 434.38005609715236\n",
      "Epoch: 20, Loss: 513.0692846419963\n",
      "Epoch: 21, Loss: 592.7511921334774\n",
      "Epoch: 22, Loss: 469.1528213176322\n",
      "Epoch: 23, Loss: 452.560278222916\n",
      "Epoch: 24, Loss: 447.7554191427028\n",
      "Epoch: 25, Loss: 438.38647882989113\n",
      "Epoch: 26, Loss: 433.5987190895892\n",
      "Epoch: 27, Loss: 430.7721763773167\n",
      "Epoch: 28, Loss: 418.9190811806537\n",
      "Epoch: 29, Loss: 398.0146209229814\n",
      "Epoch: 30, Loss: 383.8474717647471\n",
      "Epoch: 31, Loss: 375.6161904030658\n",
      "Epoch: 32, Loss: 386.4532172020445\n",
      "Epoch: 33, Loss: 373.84097590345016\n",
      "Epoch: 34, Loss: 340.9755987613759\n",
      "Epoch: 35, Loss: 318.71251792096075\n",
      "Epoch: 36, Loss: 307.4836037818422\n",
      "Epoch: 37, Loss: 297.1878464069772\n",
      "Epoch: 38, Loss: 289.20996454928786\n",
      "Epoch: 39, Loss: 284.068517522609\n",
      "Epoch: 40, Loss: 278.6535973244525\n",
      "Epoch: 41, Loss: 274.83884153974816\n",
      "Epoch: 42, Loss: 274.4407163579413\n",
      "Epoch: 43, Loss: 269.8054892763178\n",
      "Epoch: 44, Loss: 267.61367509720174\n",
      "Epoch: 45, Loss: 264.7719423009994\n",
      "Epoch: 46, Loss: 260.764240467802\n",
      "Epoch: 47, Loss: 259.56019612576097\n",
      "Epoch: 48, Loss: 257.9472701295893\n",
      "Epoch: 49, Loss: 253.38611217255288\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, optimizer, scheduler, epochs):\n",
    "    loss_hist = []\n",
    "    for epoch in range(epochs):\n",
    "        # training\n",
    "        model.train()\n",
    "        for batch_idx, datum in enumerate(train_loader):\n",
    "            image = datum['image'].cuda()\n",
    "            label = datum['label'].cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(image)\n",
    "            loss = output.loss\n",
    "            loss_hist.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        # avg loss \n",
    "        avg_loss = sum(loss_hist) / len(loss_hist)\n",
    "        loss_hist.clear()\n",
    "        print(f'Epoch: {epoch}, Loss: {avg_loss}')\n",
    "\n",
    "import torch\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "model = vit_mae.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.5)\n",
    "\n",
    "train(model, train_loader, optimizer, scheduler, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 more epochs\n",
    "epochs = 50\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "train(model, train_loader, optimizer, scheduler, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "from datetime import datetime\n",
    "epochs = 50\n",
    "dt = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model.save_pretrained(f'./pretrained_vit_mae_3d_25pct_{epochs}_epochs_{dt}')\n",
    "torch.save(model.state_dict(), f'./pretrained_vit_mae_25pct_{epochs}_epochs_{dt}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
